options:
  logging: CLOUD_LOGGING_ONLY
steps:
  # Step 1: Upload the new dataset to GCS
  - name: "gcr.io/cloud-builders/gsutil"
    id: "UploadDataset"
    args: ["cp", "wine.csv", "gs://yannick-pipeline-root/datasets/wine-${SHORT_SHA}.csv"]
  # Step 2: Fetch pipeline.py from main branch
  - name: "gcr.io/cloud-builders/git"
    id: "FetchPipeline"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        git fetch origin main
        git checkout origin/main -- pipeline.py
        ls -la pipeline.py
  # Step 3: Compile the pipeline
  - name: "python:3.9-slim"
    id: "CompilePipeline"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        pip install google-cloud-aiplatform kfp
        python3 pipeline.py
    waitFor: ["FetchPipeline"]
  # Step 4: Run the pipeline with the new data
  - name: "python:3.9-slim"
    id: "RunTrainingPipeline"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        pip install 'google-cloud-aiplatform[pipelines]' --break-system-packages
        python3 -c "
        from google.cloud import aiplatform
        
        aiplatform.init(project='${PROJECT_ID}', location='europe-west4')
        
        job = aiplatform.PipelineJob(
            display_name='data-triggered-run-${SHORT_SHA}',
            template_path='wine_quality_pipeline_git_triggered.yaml',
            pipeline_root='gs://yannick-pipeline-root',
            enable_caching=False,
            parameter_values={
                'input_data_gcs_path': 'gs://yannick-pipeline-root/datasets/wine-${SHORT_SHA}.csv'
            }
        )
        
        job.submit(service_account='793868790421-compute@developer.gserviceaccount.com')
        print(f'Pipeline submitted: {job.resource_name}')
        "
    waitFor: ["CompilePipeline", "UploadDataset"]